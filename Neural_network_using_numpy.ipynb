{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpofZ6bGg4TXSH6YsFSZcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palVikram/Machine-Learning-using-Python/blob/master/Neural_network_using_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ZJLRBxX3Idsh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## http://archive.ics.uci.edu/ml/datasets/banknote+authentication#\n",
        "### download data set from the above link "
      ],
      "metadata": {
        "id": "kVvldfrhS8Jq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=np.genfromtxt('/content/data_banknote_authentication.txt',delimiter=',')"
      ],
      "metadata": {
        "id": "Qb-6F0uZOiK6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[:,:4]\n",
        "y = df[:, 4]\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8z_RDG1OtLU",
        "outputId": "9473f60d-2683-411d-bc70-ac6935ec27e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
              "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
              "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
              "       ...,\n",
              "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
              "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
              "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.T\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "\n",
        "X_test = X_test.T\n",
        "y_test = y_test.reshape(1, y_test.shape[0])"
      ],
      "metadata": {
        "id": "fE3P_uuROR2v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_model(X, Y, hidden_unit, num_iterations = 1000):\n",
        "\n",
        "    input_unit = X.shape[0] # size of input layer\n",
        "    hidden_unit = 4 #hidden layer of size 4\n",
        "    output_unit = Y.shape[0] # size of output layer\n",
        "\n",
        "    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n",
        "    b1 = np.zeros((hidden_unit, 1))\n",
        "    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n",
        "    b2 = np.zeros((output_unit, 1))\n",
        "   \n",
        "    learning_rate = 0.01\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        ### forward propagation\n",
        "        Z1 = np.dot(W1, X) + b1\n",
        "        A1 = np.tanh(Z1)\n",
        "        Z2 = np.dot(W2, A1) + b2\n",
        "        A2 = 1/(1+np.exp(-Z2))\n",
        "\n",
        "        ### cross entropy cost\n",
        "        m = Y.shape[1] \n",
        "        # Compute the cross-entropy cost\n",
        "        logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
        "        cost = - np.sum(logprobs) / m\n",
        "        cost = float(np.squeeze(cost))\n",
        "\n",
        "        ### cost function\n",
        "        dZ2 = A2-Y\n",
        "        dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "        dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "        dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "        db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        ### grad update \n",
        "        W1 = W1 - learning_rate * dW1\n",
        "        b1 = b1 - learning_rate * db1\n",
        "        W2 = W2 - learning_rate * dW2\n",
        "        b2 = b2 - learning_rate * db2\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    return W1,W2,b1,b2\n",
        "\n",
        "W1,W2,b1,b2 = neural_network_model(X_train, y_train, 4, num_iterations=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iNqHrzpIzk3",
        "outputId": "bf4096d4-b7a3-4296-8be1-63034536bc28"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693161\n",
            "Cost after iteration 5: 0.692797\n",
            "Cost after iteration 10: 0.692435\n",
            "Cost after iteration 15: 0.692069\n",
            "Cost after iteration 20: 0.691693\n",
            "Cost after iteration 25: 0.691302\n",
            "Cost after iteration 30: 0.690890\n",
            "Cost after iteration 35: 0.690449\n",
            "Cost after iteration 40: 0.689974\n",
            "Cost after iteration 45: 0.689457\n",
            "Cost after iteration 50: 0.688890\n",
            "Cost after iteration 55: 0.688265\n",
            "Cost after iteration 60: 0.687572\n",
            "Cost after iteration 65: 0.686803\n",
            "Cost after iteration 70: 0.685946\n",
            "Cost after iteration 75: 0.684993\n",
            "Cost after iteration 80: 0.683932\n",
            "Cost after iteration 85: 0.682752\n",
            "Cost after iteration 90: 0.681445\n",
            "Cost after iteration 95: 0.679999\n",
            "Cost after iteration 100: 0.678405\n",
            "Cost after iteration 105: 0.676655\n",
            "Cost after iteration 110: 0.674741\n",
            "Cost after iteration 115: 0.672657\n",
            "Cost after iteration 120: 0.670397\n",
            "Cost after iteration 125: 0.667956\n",
            "Cost after iteration 130: 0.665333\n",
            "Cost after iteration 135: 0.662524\n",
            "Cost after iteration 140: 0.659529\n",
            "Cost after iteration 145: 0.656346\n",
            "Cost after iteration 150: 0.652974\n",
            "Cost after iteration 155: 0.649414\n",
            "Cost after iteration 160: 0.645663\n",
            "Cost after iteration 165: 0.641722\n",
            "Cost after iteration 170: 0.637588\n",
            "Cost after iteration 175: 0.633260\n",
            "Cost after iteration 180: 0.628736\n",
            "Cost after iteration 185: 0.624013\n",
            "Cost after iteration 190: 0.619088\n",
            "Cost after iteration 195: 0.613961\n",
            "Cost after iteration 200: 0.608629\n",
            "Cost after iteration 205: 0.603091\n",
            "Cost after iteration 210: 0.597350\n",
            "Cost after iteration 215: 0.591407\n",
            "Cost after iteration 220: 0.585266\n",
            "Cost after iteration 225: 0.578935\n",
            "Cost after iteration 230: 0.572422\n",
            "Cost after iteration 235: 0.565739\n",
            "Cost after iteration 240: 0.558897\n",
            "Cost after iteration 245: 0.551913\n",
            "Cost after iteration 250: 0.544802\n",
            "Cost after iteration 255: 0.537581\n",
            "Cost after iteration 260: 0.530269\n",
            "Cost after iteration 265: 0.522882\n",
            "Cost after iteration 270: 0.515439\n",
            "Cost after iteration 275: 0.507958\n",
            "Cost after iteration 280: 0.500455\n",
            "Cost after iteration 285: 0.492947\n",
            "Cost after iteration 290: 0.485448\n",
            "Cost after iteration 295: 0.477973\n",
            "Cost after iteration 300: 0.470534\n",
            "Cost after iteration 305: 0.463145\n",
            "Cost after iteration 310: 0.455816\n",
            "Cost after iteration 315: 0.448558\n",
            "Cost after iteration 320: 0.441379\n",
            "Cost after iteration 325: 0.434286\n",
            "Cost after iteration 330: 0.427289\n",
            "Cost after iteration 335: 0.420391\n",
            "Cost after iteration 340: 0.413599\n",
            "Cost after iteration 345: 0.406917\n",
            "Cost after iteration 350: 0.400348\n",
            "Cost after iteration 355: 0.393896\n",
            "Cost after iteration 360: 0.387562\n",
            "Cost after iteration 365: 0.381348\n",
            "Cost after iteration 370: 0.375255\n",
            "Cost after iteration 375: 0.369284\n",
            "Cost after iteration 380: 0.363435\n",
            "Cost after iteration 385: 0.357707\n",
            "Cost after iteration 390: 0.352101\n",
            "Cost after iteration 395: 0.346615\n",
            "Cost after iteration 400: 0.341247\n",
            "Cost after iteration 405: 0.335998\n",
            "Cost after iteration 410: 0.330864\n",
            "Cost after iteration 415: 0.325845\n",
            "Cost after iteration 420: 0.320938\n",
            "Cost after iteration 425: 0.316142\n",
            "Cost after iteration 430: 0.311454\n",
            "Cost after iteration 435: 0.306872\n",
            "Cost after iteration 440: 0.302394\n",
            "Cost after iteration 445: 0.298019\n",
            "Cost after iteration 450: 0.293743\n",
            "Cost after iteration 455: 0.289564\n",
            "Cost after iteration 460: 0.285481\n",
            "Cost after iteration 465: 0.281490\n",
            "Cost after iteration 470: 0.277590\n",
            "Cost after iteration 475: 0.273778\n",
            "Cost after iteration 480: 0.270052\n",
            "Cost after iteration 485: 0.266410\n",
            "Cost after iteration 490: 0.262850\n",
            "Cost after iteration 495: 0.259370\n",
            "Cost after iteration 500: 0.255967\n",
            "Cost after iteration 505: 0.252640\n",
            "Cost after iteration 510: 0.249386\n",
            "Cost after iteration 515: 0.246204\n",
            "Cost after iteration 520: 0.243092\n",
            "Cost after iteration 525: 0.240047\n",
            "Cost after iteration 530: 0.237068\n",
            "Cost after iteration 535: 0.234154\n",
            "Cost after iteration 540: 0.231302\n",
            "Cost after iteration 545: 0.228511\n",
            "Cost after iteration 550: 0.225779\n",
            "Cost after iteration 555: 0.223104\n",
            "Cost after iteration 560: 0.220486\n",
            "Cost after iteration 565: 0.217922\n",
            "Cost after iteration 570: 0.215411\n",
            "Cost after iteration 575: 0.212952\n",
            "Cost after iteration 580: 0.210543\n",
            "Cost after iteration 585: 0.208182\n",
            "Cost after iteration 590: 0.205870\n",
            "Cost after iteration 595: 0.203604\n",
            "Cost after iteration 600: 0.201382\n",
            "Cost after iteration 605: 0.199205\n",
            "Cost after iteration 610: 0.197070\n",
            "Cost after iteration 615: 0.194977\n",
            "Cost after iteration 620: 0.192925\n",
            "Cost after iteration 625: 0.190912\n",
            "Cost after iteration 630: 0.188937\n",
            "Cost after iteration 635: 0.187000\n",
            "Cost after iteration 640: 0.185100\n",
            "Cost after iteration 645: 0.183235\n",
            "Cost after iteration 650: 0.181404\n",
            "Cost after iteration 655: 0.179608\n",
            "Cost after iteration 660: 0.177844\n",
            "Cost after iteration 665: 0.176112\n",
            "Cost after iteration 670: 0.174412\n",
            "Cost after iteration 675: 0.172742\n",
            "Cost after iteration 680: 0.171101\n",
            "Cost after iteration 685: 0.169490\n",
            "Cost after iteration 690: 0.167907\n",
            "Cost after iteration 695: 0.166352\n",
            "Cost after iteration 700: 0.164823\n",
            "Cost after iteration 705: 0.163321\n",
            "Cost after iteration 710: 0.161845\n",
            "Cost after iteration 715: 0.160393\n",
            "Cost after iteration 720: 0.158966\n",
            "Cost after iteration 725: 0.157563\n",
            "Cost after iteration 730: 0.156183\n",
            "Cost after iteration 735: 0.154826\n",
            "Cost after iteration 740: 0.153491\n",
            "Cost after iteration 745: 0.152178\n",
            "Cost after iteration 750: 0.150886\n",
            "Cost after iteration 755: 0.149615\n",
            "Cost after iteration 760: 0.148364\n",
            "Cost after iteration 765: 0.147133\n",
            "Cost after iteration 770: 0.145921\n",
            "Cost after iteration 775: 0.144729\n",
            "Cost after iteration 780: 0.143554\n",
            "Cost after iteration 785: 0.142398\n",
            "Cost after iteration 790: 0.141260\n",
            "Cost after iteration 795: 0.140139\n",
            "Cost after iteration 800: 0.139035\n",
            "Cost after iteration 805: 0.137947\n",
            "Cost after iteration 810: 0.136876\n",
            "Cost after iteration 815: 0.135821\n",
            "Cost after iteration 820: 0.134781\n",
            "Cost after iteration 825: 0.133756\n",
            "Cost after iteration 830: 0.132747\n",
            "Cost after iteration 835: 0.131752\n",
            "Cost after iteration 840: 0.130771\n",
            "Cost after iteration 845: 0.129804\n",
            "Cost after iteration 850: 0.128851\n",
            "Cost after iteration 855: 0.127912\n",
            "Cost after iteration 860: 0.126986\n",
            "Cost after iteration 865: 0.126073\n",
            "Cost after iteration 870: 0.125172\n",
            "Cost after iteration 875: 0.124284\n",
            "Cost after iteration 880: 0.123408\n",
            "Cost after iteration 885: 0.122544\n",
            "Cost after iteration 890: 0.121692\n",
            "Cost after iteration 895: 0.120851\n",
            "Cost after iteration 900: 0.120021\n",
            "Cost after iteration 905: 0.119203\n",
            "Cost after iteration 910: 0.118396\n",
            "Cost after iteration 915: 0.117599\n",
            "Cost after iteration 920: 0.116812\n",
            "Cost after iteration 925: 0.116036\n",
            "Cost after iteration 930: 0.115270\n",
            "Cost after iteration 935: 0.114514\n",
            "Cost after iteration 940: 0.113767\n",
            "Cost after iteration 945: 0.113031\n",
            "Cost after iteration 950: 0.112303\n",
            "Cost after iteration 955: 0.111585\n",
            "Cost after iteration 960: 0.110875\n",
            "Cost after iteration 965: 0.110175\n",
            "Cost after iteration 970: 0.109483\n",
            "Cost after iteration 975: 0.108800\n",
            "Cost after iteration 980: 0.108126\n",
            "Cost after iteration 985: 0.107459\n",
            "Cost after iteration 990: 0.106801\n",
            "Cost after iteration 995: 0.106151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X,W1,W2,b1,b2):\n",
        "        ### forward propagation\n",
        "        Z1 = np.dot(W1, X) + b1\n",
        "        A1 = np.tanh(Z1)\n",
        "        Z2 = np.dot(W2, A1) + b2\n",
        "        A2 = 1/(1+np.exp(-Z2))\n",
        "        return A2\n"
      ],
      "metadata": {
        "id": "iH0GPL7qQVhr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(W1,W2,b1,b2, X):\n",
        "    A2 = forward_propagation(X, W1,W2,b1,b2)\n",
        "    predictions = np.round(A2)\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "c_XuHDLAI30r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = prediction(W1,W2,b1,b2, X_train)\n",
        "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\n",
        "\n",
        "predictions = prediction(W1,W2,b1,b2, X_test)\n",
        "print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt0Lq3lzI7b3",
        "outputId": "4b649991-6389-413b-c1d7-38b18a70c450"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train: 97%\n",
            "Accuracy Test: 96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HsX6SjGXQ1Ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}